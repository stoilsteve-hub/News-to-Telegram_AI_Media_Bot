import os
import re
import html
import time
import sqlite3
import asyncio
import difflib
import traceback
from io import BytesIO
from urllib.parse import quote_plus, urljoin, urlparse

import requests
import feedparser
from dotenv import load_dotenv
from openai import OpenAI

from telegram import Update, InputFile
from telegram.constants import ParseMode
from telegram.ext import Application, CommandHandler, ContextTypes

# ============================================================
# ENV
# ============================================================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_PATH = os.path.join(BASE_DIR, "config", ".env")
load_dotenv(ENV_PATH)

BOT_TOKEN = (os.getenv("BOT_TOKEN") or "").strip()
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()
EDITOR_CHAT_ID = int((os.getenv("EDITOR_CHAT_ID") or "0").strip())
PUBLIC_CHANNEL_ID = int((os.getenv("PUBLIC_CHANNEL_ID") or "0").strip())
TELEGRAM_HANDLE = (os.getenv("TELEGRAM_HANDLE") or "@SWEzhach0k").strip()

JOB_TICK_SECONDS = int((os.getenv("JOB_TICK_SECONDS") or "360").strip())
RUN_COOLDOWN_SECONDS = int((os.getenv("RUN_COOLDOWN_SECONDS") or "300").strip())

PER_FEED_CAP = int((os.getenv("PER_FEED_CAP") or "10").strip())
MAX_PER_RUN = int((os.getenv("MAX_PER_RUN") or "1").strip())
MIN_SCORE = int((os.getenv("MIN_SCORE") or "1").strip())

OPENAI_MODEL = (os.getenv("OPENAI_MODEL") or "gpt-4o-mini").strip()
OPENAI_MAX_TOKENS = int((os.getenv("OPENAI_MAX_TOKENS") or "650").strip())
OPENAI_TEMPERATURE = float((os.getenv("OPENAI_TEMPERATURE") or "0.2").strip())

DB_PATH = os.path.join(BASE_DIR, "posted_items.sqlite")

# turn OFF previews so Telegram doesn't show Swedish page snippets

# turn OFF previews so Telegram doesn't show Swedish page snippets
DISABLE_PREVIEWS = True
AUTO_POST = (os.getenv("AUTO_POST", "false").lower().strip() == "true")

AUTO_POST = (os.getenv("AUTO_POST", "false").lower().strip() == "true")

if not BOT_TOKEN or not OPENAI_API_KEY or not EDITOR_CHAT_ID or not PUBLIC_CHANNEL_ID:
    raise RuntimeError(
        f"Missing env vars. Check {ENV_PATH}\n"
        f"Required: BOT_TOKEN, OPENAI_API_KEY, EDITOR_CHAT_ID, PUBLIC_CHANNEL_ID"
    )

# ============================================================
# SINGLE INSTANCE LOCK
# ============================================================

LOCK_PATH = os.path.join(BASE_DIR, ".bot.lock")


def acquire_lock_or_exit() -> None:
    if os.path.exists(LOCK_PATH):
        try:
            with open(LOCK_PATH, "r", encoding="utf-8") as f:
                pid_str = f.read().strip()
            if pid_str.isdigit():
                pid = int(pid_str)
                os.kill(pid, 0)
                raise SystemExit(f"[LOCK] Another bot instance is running (PID={pid}). Stop it first.")
        except ProcessLookupError:
            try:
                os.remove(LOCK_PATH)
            except Exception:
                pass
        except Exception:
            try:
                os.remove(LOCK_PATH)
            except Exception:
                pass

    with open(LOCK_PATH, "w", encoding="utf-8") as f:
        f.write(str(os.getpid()))


def release_lock() -> None:
    try:
        if os.path.exists(LOCK_PATH):
            os.remove(LOCK_PATH)
    except Exception:
        pass


# ============================================================
# RSS
# ============================================================

feedparser.USER_AGENT = "SwedishWallBot/10.0 (+https://t.me/SWEzhach0k)"


def google_news_rss(q: str) -> str:
    return f"https://news.google.com/rss/search?q={quote_plus(q)}&hl=sv&gl=SE&ceid=SE:sv"


RSS_FEEDS = [
    ("SVT Nyheter", "https://www.svt.se/nyheter/rss.xml"),
    ("SR Ekot", "https://api.sr.se/api/rss/pod/3795"),
    ("8 Sidor", "https://8sidor.se/feed/"),
    ("Expressen Nyheter", "https://feeds.expressen.se/nyheter/"),
    ("Government.se via Google", google_news_rss("site:government.se")),
    ("TV4.se via Google", google_news_rss("site:tv4.se")),
    ("Expressen Debatt", "https://feeds.expressen.se/debatt/"),
    ("Aftonbladet", "https://rss.aftonbladet.se/rss2/small/pages/sections/senastenytt/"),
]

KEYWORDS = [
    "ukraina", "ryssland", "kriget", "putin", "zelensky",
    "nato", "f√∂rsvar", "s√§kerhet", "sverige", "svensk",
    "regeringen", "riksdagen", "saab", "gripen"
]

HOT_TERMS = [
    "akut", "varnar", "skandal", "explosion", "attack", "ryssland", "ukraina", "krig",
    "olja", "gas", "nato", "ekonomi", "bist√•nd", "sverige", "regering", "putin",
    "zelenskyj", "brott", "migration", "dr√∂nare", "kultur", "musik",
    "melodifestivalen", "eurovision", "esc", "brand", "v√•ldt√§kt", "utvisning",
    "l√•n", "r√§nta", "riksbank", "euro", "dollar", "eu"
]


def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").lower())


def score_entry(title: str, summary: str) -> int:
    text = normalize((title or "") + " " + (summary or ""))
    score = 0
    for kw in KEYWORDS:
        if kw in text:
            score += 3
    for ht in HOT_TERMS:
        if ht in text:
            score += 2
    return score


def detect_article_type(source_name: str, title: str, link: str) -> str:
    t = (source_name + " " + (title or "") + " " + (link or "")).lower()
    if any(x in t for x in ["debatt", "ledare", "opinion"]):
        return "debate"
    return "news"


def fetch_feed(url: str) -> feedparser.FeedParserDict:
    resp = requests.get(url, timeout=20, headers={"User-Agent": feedparser.USER_AGENT})
    resp.raise_for_status()
    return feedparser.parse(resp.content)


def extract_item_id(entry) -> str:
    link = (entry.get("link") or "").strip()
    eid = (entry.get("id") or entry.get("guid") or link or "").strip()
    return eid


def strip_html_text(s: str) -> str:
    s = s or ""
    s = html.unescape(s)
    s = re.sub(r"<\s*br\s*/?\s*>", "\n", s, flags=re.I)
    s = re.sub(r"</p\s*>", "\n\n", s, flags=re.I)
    s = re.sub(r"<[^>]+>", "", s)
    s = re.sub(r"\n{3,}", "\n\n", s).strip()
    s = re.sub(r"\s+", " ", s).strip()
    return s


# ============================================================
# PARSE PHOTO FROM ORIGINAL ARTICLE
# ============================================================

META_OG_IMAGE_RE = re.compile(
    r'<meta[^>]+(?:property|name)\s*=\s*["\'](?:og:image|twitter:image|twitter:image:src)["\'][^>]+content\s*=\s*["\']([^"\']+)["\']',
    re.I
)
META_OG_IMAGE_ALT_RE = re.compile(
    r'<meta[^>]+content\s*=\s*["\']([^"\']+)["\'][^>]+(?:property|name)\s*=\s*["\'](?:og:image|twitter:image|twitter:image:src)["\']',
    re.I
)
IMG_SRC_RE = re.compile(r"<img[^>]+src\s*=\s*['\"]([^'\"]+)['\"]", re.I)


def fetch_article_image(article_url: str) -> str:
    """
    Best-effort image extraction from article HTML.
    Priority:
      1) og:image / twitter:image
      2) first <img src="...">
    Returns "" if not found.
    """
    u = (article_url or "").strip()
    if not u:
        return ""

    resp = requests.get(
        u,
        timeout=20,
        headers={
            "User-Agent": feedparser.USER_AGENT,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        },
        allow_redirects=True,
    )
    resp.raise_for_status()
    html_text = resp.text or ""

    m = META_OG_IMAGE_RE.search(html_text) or META_OG_IMAGE_ALT_RE.search(html_text)
    if m:
        img = (m.group(1) or "").strip()
        if img:
            return urljoin(u, img)

    m2 = IMG_SRC_RE.search(html_text)
    if m2:
        img = (m2.group(1) or "").strip()
        if img:
            return urljoin(u, img)

    return ""


# ============================================================
# IMAGE FILTERING (Mode A: if not usable -> no image)
# ============================================================

BLOCKED_IMAGE_KEYWORDS = [
    "logo", "logos", "logotyp", "logotype", "icon", "icons", "favicon",
    "sprite", "badge", "default", "placeholder", "fallback", "share-default",
    "app-icon", "apple-touch-icon", "msapplication", "siteicon", "brand",
    "sverigesradio", "ekot_logo", "googlelogo", "google_logo"
]

BLOCKED_IMAGE_EXTENSIONS = [".svg"]

BLOCKED_IMAGE_HOSTS = {
    "news.google.com",
    "www.google.com",
}


def _looks_like_logo_url(image_url: str) -> bool:
    u = (image_url or "").strip()
    if not u:
        return True
    try:
        p = urlparse(u)
        host = (p.netloc or "").lower()
        path = (p.path or "").lower()
        query = (p.query or "").lower()
    except Exception:
        return True

    if host in BLOCKED_IMAGE_HOSTS:
        return True

    for ext in BLOCKED_IMAGE_EXTENSIONS:
        if path.endswith(ext):
            return True

    hay = f"{host} {path} {query}"
    return any(k in hay for k in BLOCKED_IMAGE_KEYWORDS)


def _image_dimensions_from_bytes(data: bytes) -> tuple[int, int]:
    """
    Returns (w,h) or (0,0) if unknown.
    Handles PNG/JPEG/GIF/WEBP enough for filtering.
    """
    if not data or len(data) < 16:
        return (0, 0)

    # PNG
    if data.startswith(b"\x89PNG\r\n\x1a\n") and len(data) >= 24:
        w = int.from_bytes(data[16:20], "big", signed=False)
        h = int.from_bytes(data[20:24], "big", signed=False)
        return (w, h)

    # GIF
    if data[:6] in (b"GIF87a", b"GIF89a") and len(data) >= 10:
        w = int.from_bytes(data[6:8], "little", signed=False)
        h = int.from_bytes(data[8:10], "little", signed=False)
        return (w, h)

    # WEBP (RIFF....WEBP)
    if data[:4] == b"RIFF" and len(data) >= 30 and data[8:12] == b"WEBP":
        # VP8X chunk gives dimensions (most reliable)
        if data[12:16] == b"VP8X" and len(data) >= 30:
            w = 1 + int.from_bytes(data[24:27], "little", signed=False)
            h = 1 + int.from_bytes(data[27:30], "little", signed=False)
            return (w, h)
        return (0, 0)

    # JPEG: scan for SOF markers
    if data.startswith(b"\xff\xd8"):
        i = 2
        n = len(data)
        while i + 9 < n:
            if data[i] != 0xFF:
                i += 1
                continue
            marker = data[i + 1]
            i += 2
            # skip padding FFs
            while marker == 0xFF and i < n:
                marker = data[i]
                i += 1
            # standalone markers
            if marker in (0xD8, 0xD9):
                continue
            if i + 2 > n:
                break
            seglen = int.from_bytes(data[i:i + 2], "big", signed=False)
            if seglen < 2 or i + seglen > n:
                break
            # SOF0..SOF3, SOF5..SOF7, SOF9..SOF11, SOF13..SOF15
            if marker in (0xC0, 0xC1, 0xC2, 0xC3, 0xC5, 0xC6, 0xC7, 0xC9, 0xCA, 0xCB, 0xCD, 0xCE, 0xCF):
                if i + 7 <= n:
                    h = int.from_bytes(data[i + 3:i + 5], "big", signed=False)
                    w = int.from_bytes(data[i + 5:i + 7], "big", signed=False)
                    return (w, h)
                break
            i += seglen
        return (0, 0)

    return (0, 0)


def is_usable_image(image_url: str) -> bool:
    """
    Mode A filter:
    - Reject obvious logos/placeholders by URL.
    - Reject too-small images (common for logos/icons).
    """
    u = (image_url or "").strip()
    if not u:
        return False

    if _looks_like_logo_url(u):
        return False

    # Cheap size gate: download up to a small cap just to read dimensions.
    # Uses streaming and stops early once we can likely parse headers.
    try:
        r = requests.get(
            u,
            timeout=20,
            headers={
                "User-Agent": feedparser.USER_AGENT,
                "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
                "Referer": u,
            },
            stream=True,
            allow_redirects=True,
        )
        r.raise_for_status()
        buf = b""
        max_probe = 256 * 1024  # 256KB is enough to find JPEG SOF in most cases
        for chunk in r.iter_content(chunk_size=32 * 1024):
            if not chunk:
                continue
            buf += chunk
            if len(buf) >= max_probe:
                break
            w, h = _image_dimensions_from_bytes(buf)
            if w and h:
                break

        w, h = _image_dimensions_from_bytes(buf)
        if w == 0 or h == 0:
            # If we can't parse dims, allow it (better to keep photos than drop them).
            return True

        # Basic "looks like a real article photo" gate
        if w < 420 or h < 240:
            return False

        # Many logos are near-square; photos usually aren't.
        aspect = w / h if h else 0.0
        if 0.85 <= aspect <= 1.15 and max(w, h) < 900:
            return False

        return True
    except Exception:
        # If probing fails, err on "no image" (safer: avoids broken posts).
        return False


# ============================================================
# DOWNLOAD IMAGE BYTES (so Telegram doesn't need to fetch the URL)
# ============================================================

def download_image_bytes(image_url: str, max_bytes: int = 12_000_000) -> tuple[bytes, str]:
    """
    Downloads image bytes. Returns (bytes, filename).
    Raises on non-image content or too-large downloads.
    """
    u = (image_url or "").strip()
    if not u:
        raise ValueError("empty image url")

    r = requests.get(
        u,
        timeout=25,
        headers={
            "User-Agent": feedparser.USER_AGENT,
            "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
            "Referer": u,
        },
        stream=True,
        allow_redirects=True,
    )
    r.raise_for_status()

    ct = (r.headers.get("Content-Type") or "").lower()
    chunks: list[bytes] = []
    total = 0
    for chunk in r.iter_content(chunk_size=64 * 1024):
        if not chunk:
            continue
        chunks.append(chunk)
        total += len(chunk)
        if total > max_bytes:
            raise ValueError("image too large")

    data = b"".join(chunks)

    # Validate type lightly
    if "image" not in ct:
        if not (data.startswith(b"\xff\xd8\xff") or data.startswith(b"\x89PNG") or data[:4] == b"RIFF" or data[
            :4] == b"GIF8"):
            raise ValueError(f"not an image (content-type={ct or 'unknown'})")

    # Filename guess
    ext = ".jpg"
    if "png" in ct or data.startswith(b"\x89PNG"):
        ext = ".png"
    elif "webp" in ct or (data[:4] == b"RIFF" and data[8:12] == b"WEBP"):
        ext = ".webp"
    elif "gif" in ct or data[:4] == b"GIF8":
        ext = ".gif"
    elif "jpeg" in ct or "jpg" in ct or data.startswith(b"\xff\xd8\xff"):
        ext = ".jpg"

    return data, f"photo{ext}"


# ============================================================
# HASHTAGS (core + topic-based, Russian-only, capped)
# ============================================================

HASHTAG_CORE = [
    "#–®–≤–µ—Ü–∏—è",
    "#–ù–æ–≤–æ—Å—Ç–∏",
]

HASHTAG_TOPICS = {
    "ukraine_russia": ["#–£–∫—Ä–∞–∏–Ω–∞", "#–†–æ—Å—Å–∏—è", "#–í–æ–π–Ω–∞", "#–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "#–°–∞–Ω–∫—Ü–∏–∏"],
    "nato_defense": ["#–ù–ê–¢–û", "#–û–±–æ—Ä–æ–Ω–∞", "#–ê—Ä–º–∏—è", "#–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "#–°–µ–≤–µ—Ä–Ω–∞—è–ï–≤—Ä–æ–ø–∞"],
    "politics": ["#–ü–æ–ª–∏—Ç–∏–∫–∞", "#–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "#–†–∏–∫—Å–¥–∞–≥", "#–ü–∞—Ä—Ç–∏–∏"],
    "crime": ["#–ö—Ä–∏–º–∏–Ω–∞–ª", "#–ü–æ–ª–∏—Ü–∏—è", "#–°—É–¥", "#–ü—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç—å", "#–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"],
    "migration": ["#–ú–∏–≥—Ä–∞—Ü–∏—è", "#–ë–µ–∂–µ–Ω—Ü—ã", "#–ì—Ä–∞–Ω–∏—Ü—ã", "#–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è"],
    "economy": ["#–≠–∫–æ–Ω–æ–º–∏–∫–∞", "#–ò–Ω—Ñ–ª—è—Ü–∏—è", "#–¶–µ–Ω—ã", "#–ù–∞–ª–æ–≥–∏", "#–ë—é–¥–∂–µ—Ç"],
    "rates": ["#–†–∏–∫—Å–±–∞–Ω–∫", "#–°—Ç–∞–≤–∫–∏", "#–ò–ø–æ—Ç–µ–∫–∞", "#–ö—Ä–µ–¥–∏—Ç—ã", "#–§–∏–Ω–∞–Ω—Å—ã"],
    "energy": ["#–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "#–ì–∞–∑", "#–ù–µ—Ñ—Ç—å", "#–≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ", "#–ö–ª–∏–º–∞—Ç"],
    "eu": ["#–ï–°", "#–ï–≤—Ä–æ–ø–∞", "#–ë—Ä—é—Å—Å–µ–ª—å"],
    "foreign": ["#–î–∏–ø–ª–æ–º–∞—Ç–∏—è", "#–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è", "#–ì–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞"],
    "tech": ["#–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "#–ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "#–ò–ò", "#–ò–Ω—Ç–µ—Ä–Ω–µ—Ç"],
    "health": ["#–ó–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ", "#–ú–µ–¥–∏—Ü–∏–Ω–∞", "#–ë–æ–ª—å–Ω–∏—Ü—ã", "#–í—Ä–∞—á–∏"],
    "education": ["#–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "#–®–∫–æ–ª–∞", "#–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç"],
    "society": ["#–û–±—â–µ—Å—Ç–≤–æ", "#–°–æ—Ü–∏–∞–ª—å–Ω–∞—è–ü–æ–ª–∏—Ç–∏–∫–∞", "#–ü—Ä–∞–≤–∞"],
    "environment": ["#–≠–∫–æ–ª–æ–≥–∏—è", "#–ö–ª–∏–º–∞—Ç", "#–ü–æ–≥–æ–¥–∞"],
    "transport": ["#–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "#–ñ–î", "#–ê–≤–∏–∞—Å–æ–æ–±—â–µ–Ω–∏–µ", "#–î–æ—Ä–æ–≥–∏"],
    "culture": ["#–ö—É–ª—å—Ç—É—Ä–∞", "#–ú—É–∑—ã–∫–∞", "#–ö–∏–Ω–æ", "#–¢–µ–ª–µ–≤–∏–¥–µ–Ω–∏–µ", "#–ú–µ–ª–æ–¥–∏—Ñ–µ—Å—Ç–∏–≤–∞–ª–µ–Ω", "#–ï–≤—Ä–æ–≤–∏–¥–µ–Ω–∏–µ"],
    "sports": ["#–°–ø–æ—Ä—Ç", "#–§—É—Ç–±–æ–ª", "#–•–æ–∫–∫–µ–π"],
}

TOPIC_TRIGGERS = {
    "ukraine_russia": [
        "—É–∫—Ä–∞–∏–Ω", "–∫–∏–µ–≤", "–∑–µ–ª–µ–Ω—Å–∫", "—Ä–æ—Å—Å–∏", "–ø—É—Ç–∏–Ω", "–≤–æ–π–Ω",
        "ukraina", "ryssland", "zelensky", "putin", "krig"
    ],
    "nato_defense": [
        "nato", "–Ω–∞—Ç–æ", "–æ–±–æ—Ä", "–∞—Ä–º", "saab", "gripen",
        "f√∂rsvar", "s√§kerhet", "f√∂rsvaret", "f√∂rsvars"
    ],
    "politics": [
        "–ø—Ä–∞–≤–∏—Ç–µ–ª—å", "–º–∏–Ω–∏—Å—Ç—Ä", "–ø–∞—Ä–ª–∞–º–µ–Ω—Ç", "—Ä–∏–∫—Å–¥–∞–≥", "–≤—ã–±–æ—Ä",
        "regering", "riksdag", "val", "minister"
    ],
    "crime": [
        "—É–±–∏–π—Å—Ç–≤", "–≤–∑—Ä—ã–≤", "—Å—Ç—Ä–µ–ª—å–±", "–Ω–∞—Å–∏–ª", "—Å—É–¥", "–ø–æ–ª–∏—Ü–∏",
        "brott", "polis", "skjut", "explosion", "v√•ldt√§kt", "domstol"
    ],
    "migration": [
        "–º–∏–≥—Ä–∞—Ü", "–±–µ–∂–µ–Ω—Ü", "—É–±–µ–∂–∏—â", "–¥–µ–ø–æ—Ä—Ç–∞—Ü",
        "migration", "asyl", "utvis", "flykting"
    ],
    "economy": [
        "—ç–∫–æ–Ω–æ–º", "–∏–Ω—Ñ–ª—è—Ü", "—Ü–µ–Ω—ã", "–Ω–∞–ª–æ–≥", "–±—é–¥–∂–µ—Ç",
        "ekonomi", "inflation", "pris", "skatt", "budget"
    ],
    "rates": [
        "—Ä–∏–∫—Å–±–∞–Ω–∫", "—Å—Ç–∞–≤–∫", "–∏–ø–æ—Ç–µ–∫", "–∫—Ä–µ–¥–∏—Ç", "–ø—Ä–æ—Ü–µ–Ω—Ç",
        "riksbank", "r√§nta", "l√•n", "bol√•n"
    ],
    "energy": [
        "–≥–∞–∑", "–Ω–µ—Ñ—Ç—å", "—ç–ª–µ–∫—Ç—Ä", "—ç–Ω–µ—Ä–≥",
        "gas", "olja", "elpris", "energi"
    ],
    "eu": ["–µ—Å", "–µ–≤—Ä–æ—Å–æ—é–∑", "eu", "europarlament"],
    "foreign": ["–¥–∏–ø–ª–æ–º", "–ø–µ—Ä–µ–≥–æ–≤–æ—Ä", "–ø–æ—Å–æ–ª", "–≥–µ–æ–ø–æ–ª–∏—Ç", "utrikes", "diplomati"],
    "tech": [
        "–∫–∏–±–µ—Ä", "—Ö–∞–∫–µ—Ä", "—É—Ç–µ—á–∫", "–∏–∏", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω",
        "cyber", "hack", "l√§cka", "ai"
    ],
    "health": ["–º–µ–¥–∏—Ü–∏–Ω", "–±–æ–ª—å–Ω–∏—Ü", "–≤—Ä–∞—á", "covid", "sjukhus", "v√•rd"],
    "education": ["—à–∫–æ–ª", "—É–Ω–∏–≤–µ—Ä—Å", "student", "skola", "universitet"],
    "society": ["–æ–±—â–µ", "—Å–æ—Ü–∏–∞–ª", "–ø–æ—Å–æ–±–∏", "v√§lf√§rd", "social"],
    "environment": ["–∫–ª–∏–º–∞—Ç", "—ç–∫–æ–ª–æ–≥", "–ø–æ–≥–æ–¥", "klimat", "milj√∂", "v√§der"],
    "transport": ["—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç", "–ø–æ–µ–∑–¥", "—Å–∞–º–æ–ª–µ—Ç", "trafik", "t√•g", "flyg"],
    "culture": ["–∫—É–ª—å—Ç—É—Ä", "–∫–∏–Ω–æ", "–º—É–∑—ã–∫", "–µ–≤—Ä–æ–≤–∏–¥–µ–Ω", "melodifestivalen", "eurovision", "esc"],
    "sports": ["—Å–ø–æ—Ä—Ç", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ", "sport", "fotboll", "hockey"],
}


def pick_hashtags(rss_title: str, rss_summary: str, source: str) -> list[str]:
    """
    Picks a compact set of relevant hashtags based on RSS text.
    Capped to keep posts clean.
    """
    t = normalize((rss_title or "") + " " + (rss_summary or "") + " " + (source or ""))

    tags: list[str] = []
    tags.extend(HASHTAG_CORE)

    matched_topics = []
    for topic, triggers in TOPIC_TRIGGERS.items():
        if any(tr in t for tr in triggers):
            matched_topics.append(topic)

    for topic in matched_topics[:3]:
        tags.extend(HASHTAG_TOPICS.get(topic, []))

    # Deduplicate while preserving order
    seen = set()
    uniq = []
    for tag in tags:
        if tag not in seen:
            seen.add(tag)
            uniq.append(tag)

    return uniq[:12]


# ============================================================
# DB
# ============================================================

def utc_now_iso() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def _ensure_column(conn: sqlite3.Connection, table: str, col: str, ddl_type: str) -> None:
    cur = conn.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    cols = [r[1] for r in cur.fetchall()]
    if col not in cols:
        conn.execute(f"ALTER TABLE {table} ADD COLUMN {col} {ddl_type}")
        conn.commit()


def init_db() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS drafts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            created_at TEXT NOT NULL,
            text TEXT NOT NULL,
            status TEXT NOT NULL,
            error TEXT
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS posted (
            item_id TEXT PRIMARY KEY,
            posted_at TEXT NOT NULL,
            title TEXT
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS failures (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            created_at TEXT NOT NULL,
            source TEXT,
            item_id TEXT,
            stage TEXT NOT NULL,
            error TEXT NOT NULL
        )
    """)
    conn.commit()

    # Store parsed image URL (non-breaking migration)
    _ensure_column(conn, "drafts", "image_url", "TEXT")
    _ensure_column(conn, "posted", "title", "TEXT")

    return conn


def already_posted(conn: sqlite3.Connection, item_id: str) -> bool:
    c = conn.cursor()
    c.execute("SELECT 1 FROM posted WHERE item_id=?", (item_id,))
    return c.fetchone() is not None


def get_recent_titles(conn: sqlite3.Connection, limit: int = 100) -> list[str]:
    c = conn.cursor()
    c.execute("SELECT title FROM posted WHERE title IS NOT NULL AND title != '' ORDER BY posted_at DESC LIMIT ?",
              (limit,))
    return [row[0] for row in c.fetchall()]


def is_similar(text1: str, text2: str, threshold: float = 0.7) -> bool:
    """Check if two strings are similar using SequenceMatcher."""
    if not text1 or not text2:
        return False
    # Normalize for comparison
    t1 = text1.lower().strip()
    t2 = text2.lower().strip()
    if t1 == t2:
        return True
    ratio = difflib.SequenceMatcher(None, t1, t2).ratio()
    return ratio >= threshold


def mark_posted(conn: sqlite3.Connection, item_id: str, title: str = "") -> None:
    conn.execute("INSERT OR IGNORE INTO posted (item_id, posted_at, title) VALUES (?, ?, ?)",
                 (item_id, utc_now_iso(), title))
    conn.commit()


def save_failure(conn: sqlite3.Connection, source: str, item_id: str, stage: str, error: str) -> None:
    conn.execute(
        "INSERT INTO failures (created_at, source, item_id, stage, error) VALUES (?, ?, ?, ?, ?)",
        (utc_now_iso(), source, item_id, stage, (error or "")[:2000])
    )
    conn.commit()


def save_draft(conn: sqlite3.Connection, msg_html: str, status: str = "pending", error: str | None = None,
               image_url: str = "") -> int:
    cur = conn.cursor()
    cur.execute(
        "INSERT INTO drafts (created_at, text, status, error, image_url) VALUES (?, ?, ?, ?, ?)",
        (utc_now_iso(), msg_html, status, error, (image_url or "").strip())
    )
    conn.commit()
    return int(cur.lastrowid)


# ============================================================
# TELEGRAM HELPERS
# ============================================================

def hard_clip(text: str, max_len: int = 3800) -> str:
    if len(text) <= max_len:
        return text
    return text[: max_len - 20] + "\n\n‚Ä¶(truncated)"


def strip_html_tags(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "")


async def send_html_safe(bot, chat_id: int, html_text: str) -> None:
    html_text = hard_clip(html_text, 3800)
    try:
        await bot.send_message(
            chat_id=chat_id,
            text=html_text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=DISABLE_PREVIEWS,
        )
    except Exception:
        plain = hard_clip(strip_html_tags(html_text), 3800)
        await bot.send_message(
            chat_id=chat_id,
            text=plain,
        )


async def publish_to_channel(bot, chat_id: int, text: str, image_url: str = "") -> None:
    """
    Publishes text + optional image to the given chat/channel.
    Handles image download/upload fallback.
    """
    image_url = (image_url or "").strip()

    # MODE A:
    # - If we have a usable photo: attach it.
    # - Otherwise: post text only (no photo).
    if image_url:
        try:
            # Fast path: Telegram fetches URL
            await bot.send_photo(
                chat_id=chat_id,
                photo=image_url
            )
        except Exception as e_url:
            msg = str(e_url)
            # If Telegram can't fetch the URL, upload bytes ourselves
            if "failed to get http url content" in msg.lower():
                try:
                    data, fname = download_image_bytes(image_url)
                    bio = BytesIO(data)
                    bio.name = fname
                    await bot.send_photo(
                        chat_id=chat_id,
                        photo=InputFile(bio, filename=fname)
                    )
                except Exception:
                    # If image upload fails, just continue with text only
                    pass
            else:
                # Any other photo failure -> continue with text only
                pass

    await bot.send_message(
        chat_id=chat_id,
        text=hard_clip(text, 3900),
        parse_mode=ParseMode.HTML,
        disable_web_page_preview=DISABLE_PREVIEWS
    )


def make_clickable_read_link(url: str) -> str:
    u = (url or "").strip()
    if not u:
        return ""
    u_esc = html.escape(u, quote=True)
    return f'<a href="{u_esc}">–ß–∏—Ç–∞—Ç—å</a>'


def build_message_html(headline: str, summary: str, details: str, source: str, link: str, rss_title: str,
                       rss_summary: str, ai_hashtags: list[str] = None) -> str:
    h = html.escape((headline or "").strip())
    s = html.escape((summary or "").strip())
    d = html.escape((details or "").strip())
    src = html.escape((source or "").strip())
    read = make_clickable_read_link(link)

    # Use AI hashtags if available, otherwise fall back to old logic
    if ai_hashtags:
        # Ensure #–®–≤–µ—Ü–∏—è and #–ù–æ–≤–æ—Å—Ç–∏ are always present
        core = set(HASHTAG_CORE)
        final_tags = []
        # Add AI tags first
        for t in ai_hashtags:
            if not t.startswith("#"):
                t = "#" + t
            final_tags.append(t)

        # Add core tags if missing
        for c in HASHTAG_CORE:
            if c not in final_tags:
                final_tags.append(c)

        hashtags = " ".join(final_tags[:12])  # safe cap
    else:
        hashtags = " ".join(pick_hashtags(rss_title, rss_summary, source))

    return (
        f"{h}\n\n"
        f"{s}\n\n"
        f"<blockquote expandable>{d}</blockquote>\n\n"
        f"<b>Source:</b> {src}\n"
        f"<b>Link:</b> {read}\n\n"
        f"{hashtags}\n"
        f"{html.escape(TELEGRAM_HANDLE)}"
    )


# ============================================================
# OPENAI
# ============================================================

CYRILLIC_RE = re.compile(r"[–ê-–Ø–∞-—è–Å—ë]")


def is_russian_enough(text: str) -> bool:
    letters = re.findall(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë]", text or "")
    if not letters:
        return False
    cyr = sum(1 for ch in letters if CYRILLIC_RE.match(ch))
    return (cyr / len(letters)) >= 0.80


def has_latin_words(text: str) -> bool:
    return bool(re.search(r"\b[A-Za-z]{2,}\b", text or ""))


def extract_block(raw: str, label: str) -> str:
    m = re.search(rf"{label}:\s*\n(.*?)(?=\n[A-Z]+:\s*\n|\Z)", raw, flags=re.S)
    return (m.group(1).strip() if m else "")


RATE_WAIT_RE = re.compile(r"try again in\s+(\d+)m(\d+(?:\.\d+)?)s", re.I)


def parse_rate_limit_wait_seconds(msg: str) -> int:
    m = RATE_WAIT_RE.search(msg or "")
    if not m:
        return 60
    minutes = int(m.group(1))
    seconds = float(m.group(2))
    return max(10, int(minutes * 60 + seconds) + 3)


def openai_strict_three_blocks(client: OpenAI, source: str, title: str, rss_summary: str, link: str,
                               article_type: str) -> str:
    if article_type == "news":
        format_rules = (
            "- HEADLINE: 1 line, starts with exactly 1 emoji, 6‚Äì14 words.\n"
            "- SUMMARY: 2‚Äì4 sentences, 70‚Äì110 words.\n"
            "- DETAIL: 8‚Äì12 short lines, 900‚Äì1600 characters.\n"
            "- HASHTAGS: 3-6 space-separated tags (e.g. #Sweden #News).\n"
        )
    else:
        format_rules = (
            "- HEADLINE: 1 line, starts with exactly 1 emoji, 6‚Äì14 words.\n"
            "- SUMMARY: 2 short paragraphs, 80‚Äì150 words total.\n"
            "- DETAILS: 10‚Äì14 short lines, 1000‚Äì1800 characters.\n"
            "- HASHTAGS: 3-6 space-separated tags.\n"
        )

    prompt = f"""
–¢—ã –ø–∏—à–µ—à—å –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ Telegram-–∫–∞–Ω–∞–ª–∞ –æ –®–≤–µ—Ü–∏–∏.

–°–¢–†–û–ì–û:
- –ü–∏—à–∏ –¢–û–õ–¨–ö–û –ø–æ-—Ä—É—Å—Å–∫–∏ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞). –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ/—à–≤–µ–¥—Å–∫–∏–µ —Å–ª–æ–≤–∞.
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –µ—Å—Ç—å –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ/–æ–ø–∏—Å–∞–Ω–∏–∏ RSS.
- –ù–µ —Ü–∏—Ç–∏—Ä—É–π –¥–æ—Å–ª–æ–≤–Ω–æ.
- –ù–∏–∫–∞–∫–æ–≥–æ Markdown/HTML. –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç.
- –í–µ—Ä–Ω–∏ –†–û–í–ù–û 4 –±–ª–æ–∫–∞ —Å —Ç–æ—á–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏: HEADLINE / SUMMARY / DETAILS / HASHTAGS.
- –ù–∏–∫–∞–∫–∏—Ö –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–æ–∫ –¥–æ/–ø–æ—Å–ª–µ –±–ª–æ–∫–æ–≤.

HEADLINE:
...

SUMMARY:
...

DETAILS:
...

HASHTAGS:
...

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
{format_rules}

–ò—Å—Ç–æ—á–Ω–∏–∫: {source}
–ó–∞–≥–æ–ª–æ–≤–æ–∫ RSS: {title}
–û–ø–∏—Å–∞–Ω–∏–µ RSS: {rss_summary}
–°—Å—ã–ª–∫–∞: {link}
""".strip()

    r = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system",
             "content": "Return Russian only. Exactly 4 blocks: HEADLINE, SUMMARY, DETAILS, HASHTAGS."},
            {"role": "user", "content": prompt},
        ],
        temperature=OPENAI_TEMPERATURE,
        max_tokens=OPENAI_MAX_TOKENS,
    )
    return (r.choices[0].message.content or "").strip()


def openai_translate_compose(client: OpenAI, title: str, rss_summary: str, article_type: str) -> tuple[
    str, str, str, list[str]]:
    """
    Guaranteed fallback: translate/compose without requiring block labels.
    Returns (headline, summary, details, hashtags_list) in Russian.
    """
    t = (title or "").strip()
    s = (rss_summary or "").strip()

    prompt = f"""
–°–¥–µ–ª–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º (—Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞), –±–µ–∑ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö/—à–≤–µ–¥—Å–∫–∏—Ö —Å–ª–æ–≤.

–î–∞–Ω–æ:
- –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ —à–≤–µ–¥—Å–∫–æ–º/–∞–Ω–≥–ª–∏–π—Å–∫–æ–º): {t}
- –û–ø–∏—Å–∞–Ω–∏–µ RSS (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ —à–≤–µ–¥—Å–∫–æ–º/–∞–Ω–≥–ª–∏–π—Å–∫–æ–º): {s}

–°–¥–µ–ª–∞–π:
1) –ó–∞–≥–æ–ª–æ–≤–æ–∫ (1 —Å—Ç—Ä–æ–∫–∞) ‚Äî –Ω–∞—á–Ω–∏ —Å –æ–¥–Ω–æ–≥–æ —ç–º–æ–¥–∑–∏.
2) –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (2‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, 70‚Äì110 —Å–ª–æ–≤).
3) –î–µ—Ç–∞–ª–∏ (8‚Äì12 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫, –±–µ–∑ —Å—Å—ã–ª–æ–∫, –±–µ–∑ HTML/Markdown).
4) –•–µ—à—Ç–µ–≥–∏ (3-5 —à—Ç—É–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª).

–¢–∏–ø: {"–Ω–æ–≤–æ—Å—Ç—å" if article_type == "news" else "–º–Ω–µ–Ω–∏–µ/–¥–µ–±–∞—Ç—ã"}.

–í–µ—Ä–Ω–∏ –≤ —Ç–∞–∫–æ–º –≤–∏–¥–µ:
–ó–ê–ì–û–õ–û–í–û–ö: ...
–†–ï–ó–Æ–ú–ï: ...
–î–ï–¢–ê–õ–ò:
- ...
- ...
–•–ï–®–¢–ï–ì–ò: ...
""".strip()

    r = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": "Russian Cyrillic only. No Latin words. No HTML."},
            {"role": "user", "content": prompt},
        ],
        temperature=OPENAI_TEMPERATURE,
        max_tokens=OPENAI_MAX_TOKENS,
    )
    raw = (r.choices[0].message.content or "").strip()

    headline = ""
    summary = ""
    details = ""

    m1 = re.search(r"–ó–ê–ì–û–õ–û–í–û–ö:\s*(.+)", raw)
    m2 = re.search(r"–†–ï–ó–Æ–ú–ï:\s*(.+)", raw)
    m3 = re.search(r"–î–ï–¢–ê–õ–ò:\s*(.+?)(?=\n–•–ï–®–¢–ï–ì–ò:|\Z)", raw, flags=re.S)
    m4 = re.search(r"–•–ï–®–¢–ï–ì–ò:\s*(.+)", raw)

    if m1:
        headline = m1.group(1).strip()
    if m2:
        summary = m2.group(1).strip()
    if m3:
        details = m3.group(1).strip()

    hashtags = []
    if m4:
        tags_raw = m4.group(1).strip()
        for t in tags_raw.split():
            clean_t = t.strip("#., ")
            if clean_t:
                hashtags.append(clean_t)

    details = re.sub(r"^\s*[-‚Ä¢]\s*", "‚Ä¢ ", details, flags=re.M).strip()

    # Clean up details if hashtags got stuck in there
    details = details.split("–•–ï–®–¢–ï–ì–ò:")[0].strip()

    details = re.sub(r"^\s*[-‚Ä¢]\s*", "‚Ä¢ ", details, flags=re.M).strip()

    # Fallback to raising error instead of placeholders
    if not headline or not summary or not details:
        raise ValueError("OpenAI response missing HEADLINE, SUMMARY, or DETAILS.")

    joined = f"{headline}\n{summary}\n{details}"
    if not is_russian_enough(joined) or has_latin_words(joined):
        raise ValueError("OpenAI response was not consistent Russian or contained too much Latin.")

    return headline, summary, details, hashtags


def generate_post(client: OpenAI, source: str, title: str, rss_summary_raw: str, link: str, article_type: str) -> str:
    rss_summary = strip_html_text(rss_summary_raw)

    raw = openai_strict_three_blocks(client, source, title, rss_summary, link, article_type)

    has_labels = (
            re.search(r"\bHEADLINE:\s*\n", raw) and
            re.search(r"\bSUMMARY:\s*\n", raw) and
            re.search(r"\bDETAILS:\s*\n", raw)
    )

    if has_labels:
        headline = extract_block(raw, "HEADLINE")
        summ = extract_block(raw, "SUMMARY")
        details = extract_block(raw, "DETAILS")
        hashtags_raw = extract_block(raw, "HASHTAGS")

        ai_hashtags = []
        if hashtags_raw:
            for t in hashtags_raw.split():
                clean_t = t.strip("#., ")
                if clean_t:
                    ai_hashtags.append(clean_t)

        if headline and summ and details and is_russian_enough(raw) and not has_latin_words(raw):
            # Check length to prevent "ultra short" posts
            if len(headline + summ + details) < 400:
                pass
            else:
                return build_message_html(headline, summ, details, source, link, rss_title=title,
                                          rss_summary=rss_summary, ai_hashtags=ai_hashtags)

    headline, summ, details, ai_hashtags = openai_translate_compose(client, title, rss_summary, article_type)

    # Final length check
    if len(headline + summ + details) < 400:
        raise ValueError(f"Generated content too short ({len(headline + summ + details)} chars).")

    return build_message_html(headline, summ, details, source, link, rss_title=title, rss_summary=rss_summary,
                              ai_hashtags=ai_hashtags)


# ============================================================
# RSS RUN
# ============================================================

async def run_rss_once(app: Application, reason: str = "tick") -> None:
    bot = app.bot
    client: OpenAI = app.bot_data["openai_client"]
    conn: sqlite3.Connection = app.bot_data["db_conn"]

    now = time.time()
    last_run = float(app.bot_data.get("last_run_time", 0.0))
    if now - last_run < RUN_COOLDOWN_SECONDS:
        return
    app.bot_data["last_run_time"] = now

    next_ai_time = float(app.bot_data.get("next_ai_time", 0.0))
    if now < next_ai_time:
        return

    recent_titles = get_recent_titles(conn)
    candidates = []
    for source, url in RSS_FEEDS:
        try:
            feed = fetch_feed(url)
        except Exception as e:
            print(f"[RSS] fetch failed {source}: {e}", flush=True)
            save_failure(conn, source, "", "fetch_feed", str(e))
            continue

        for entry in (feed.entries or [])[:PER_FEED_CAP]:
            title = (entry.get("title") or "").strip()
            summ = (entry.get("summary") or entry.get("description") or "").strip()
            link = (entry.get("link") or "").strip()
            item_id = extract_item_id(entry)
            if not item_id:
                continue
            if already_posted(conn, item_id):
                continue

            s = score_entry(title, summ)
            if s < MIN_SCORE:
                continue

            # Deduplication: check against recent database titles
            if any(is_similar(title, rt) for rt in recent_titles):
                print(f"[RSS] skipping similar (to DB): {title} ({source})", flush=True)
                continue

            candidates.append((s, source, title, summ, link, item_id))

    # Internal candidates deduplication (same run, different sources)
    candidates.sort(key=lambda x: x[0], reverse=True)
    unique_candidates = []
    seen_titles_this_run = []
    for cand in candidates:
        s, source, title, summ, link, item_id = cand
        if any(is_similar(title, st) for st in seen_titles_this_run):
            print(f"[RSS] skipping similar (in run): {title} ({source})", flush=True)
            continue
        unique_candidates.append(cand)
        seen_titles_this_run.append(title)
    candidates = unique_candidates

    if not candidates:
        print(f"[RSS] candidates=0 (reason={reason})", flush=True)
        return

    produced = 0
    dropped = 0

    for s, source, title, summ, link, item_id in candidates[:max(1, MAX_PER_RUN)]:
        article_type = detect_article_type(source, title, link)

        # Extract image URL (best-effort)
        image_url = ""
        try:
            image_url = fetch_article_image(link)
        except Exception as ie:
            print(f"[IMG] fetch failed {source}: {ie}", flush=True)
            save_failure(conn, source, item_id, "fetch_image", str(ie))
            image_url = ""

        # Filter image: if not usable -> store empty (Mode A behavior)
        if image_url and not is_usable_image(image_url):
            image_url = ""

        try:
            msg_html = generate_post(client, source, title, summ, link, article_type)
        except Exception as ex:
            msg = str(ex)
            if "rate limit" in msg.lower() or "429" in msg:
                wait = parse_rate_limit_wait_seconds(msg)
                app.bot_data["next_ai_time"] = time.time() + wait
                print(f"[RATE LIMIT] pausing AI for {wait}s", flush=True)
                try:
                    await bot.send_message(
                        chat_id=EDITOR_CHAT_ID,
                        text=f"‚è≥ OpenAI rate limit. Pausing generation for ~{max(1, wait // 60)} minutes.",
                        disable_web_page_preview=True
                    )
                except Exception:
                    pass
                return

            dropped += 1
            print(f"[DROP] generate_post {source}: {ex}", flush=True)
            save_failure(conn, source, item_id, "generate_post", f"{ex}\n{traceback.format_exc()}")
            continue

        if AUTO_POST:
            # Auto-post immediately
            draft_id = save_draft(conn, msg_html, status="posted", image_url=image_url)
            try:
                await publish_to_channel(bot, PUBLIC_CHANNEL_ID, msg_html, image_url)
                print(f"[AUTO] posted draft #{draft_id} for {source}", flush=True)

                # Optional: log to editor chat
                await bot.send_message(chat_id=EDITOR_CHAT_ID, text=f"üöÄ Auto-posted #{draft_id} from {source}")
            except Exception as pe:
                dropped += 1
                err = f"autopost_failed: {pe}"
                print(f"[DROP] autopost {source}: {err}", flush=True)
                save_failure(conn, source, item_id, "autopost", err)
                conn.execute("UPDATE drafts SET status='failed', error=? WHERE id=?", (err[:2000], draft_id))
                conn.commit()
                continue
        else:
            # Manual approval flow
            draft_id = save_draft(conn, msg_html, status="pending", image_url=image_url)
            editor_payload = f"üìù Draft #{draft_id}\n\n{msg_html}\n\n/post {draft_id} | /skip {draft_id}"

            try:
                await send_html_safe(bot, EDITOR_CHAT_ID, editor_payload)
            except Exception as te:
                dropped += 1
                err = f"telegram_send_failed: {te}"
                print(f"[DROP] send editor {source}: {err}", flush=True)
                save_failure(conn, source, item_id, "send_editor", err)
                conn.execute("UPDATE drafts SET status='failed', error=? WHERE id=?", (err[:2000], draft_id))
                conn.commit()
                continue

        mark_posted(conn, item_id, title=title)
        produced += 1
        await asyncio.sleep(1.0)

    print(f"[RSS] produced={produced} dropped={dropped} candidates={len(candidates)} (reason={reason})", flush=True)


# ============================================================
# JOB + COMMANDS
# ============================================================

async def rss_job_callback(context: ContextTypes.DEFAULT_TYPE) -> None:
    app: Application = context.application
    print("[JOB] rss_job_callback fired", flush=True)
    try:
        await run_rss_once(app, reason="tick")
    except Exception as e:
        print(f"[RSS] job error: {e}", flush=True)


async def cmd_run(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message:
        return
    await update.message.reply_text("üîÑ Running RSS now‚Ä¶")
    await run_rss_once(context.application, reason="manual")
    await update.message.reply_text("‚úÖ RSS run complete.")


async def cmd_status(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message:
        return
    app = context.application
    conn: sqlite3.Connection | None = app.bot_data.get("db_conn")
    next_ai_time = float(app.bot_data.get("next_ai_time", 0.0))
    wait = max(0, int(next_ai_time - time.time()))

    pending = posted = failed = failures = 0
    if conn:
        c = conn.cursor()
        c.execute("SELECT COUNT(*) FROM drafts WHERE status='pending'")
        pending = int(c.fetchone()[0])
        c.execute("SELECT COUNT(*) FROM drafts WHERE status='posted'")
        posted = int(c.fetchone()[0])
        c.execute("SELECT COUNT(*) FROM drafts WHERE status='failed'")
        failed = int(c.fetchone()[0])
        c.execute("SELECT COUNT(*) FROM failures")
        failures = int(c.fetchone()[0])

    await update.message.reply_text(
        "ü§ñ Status\n"
        f"- job_tick: {JOB_TICK_SECONDS}s\n"
        f"- MAX_PER_RUN: {MAX_PER_RUN}\n"
        f"- PER_FEED_CAP: {PER_FEED_CAP}\n"
        f"- MIN_SCORE: {MIN_SCORE}\n"
        f"- model: {OPENAI_MODEL}\n"
        f"- max_tokens: {OPENAI_MAX_TOKENS}\n"
        f"- rate_limit_wait: {wait}s\n"
        f"- drafts: pending={pending} posted={posted} failed={failed}\n"
        f"- failures table: {failures}\n"
        f"- previews_disabled: {DISABLE_PREVIEWS}\n"
        f"- AUTO_POST: {AUTO_POST}\n"
    )


async def cmd_queue(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message:
        return
    conn = sqlite3.connect(DB_PATH)
    try:
        c = conn.cursor()
        c.execute("SELECT id, created_at FROM drafts WHERE status='pending' ORDER BY id DESC LIMIT 10")
        rows = c.fetchall()
        if not rows:
            await update.message.reply_text("Queue is empty (no pending drafts).")
            return
        lines = ["üóÇ Pending drafts:"]
        for did, created_at in rows:
            lines.append(f"- #{did} ({created_at}) -> /post {did} | /skip {did}")
        await update.message.reply_text("\n".join(lines))
    finally:
        conn.close()


async def cmd_post(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message:
        return
    if not context.args or not context.args[0].isdigit():
        await update.message.reply_text("Usage: /post <draft_id>")
        return

    did = int(context.args[0])
    conn = sqlite3.connect(DB_PATH)
    try:
        c = conn.cursor()
        c.execute("SELECT text, status, COALESCE(image_url,'') FROM drafts WHERE id=?", (did,))
        row = c.fetchone()
        if not row:
            await update.message.reply_text(f"Draft #{did} not found.")
            return
        text, status, image_url = row
        if status != "pending":
            await update.message.reply_text(f"Draft #{did} is not pending (status: {status}).")
            return

        image_url = (image_url or "").strip()

        await publish_to_channel(context.bot, PUBLIC_CHANNEL_ID, text, image_url)

        c.execute("UPDATE drafts SET status='posted' WHERE id=?", (did,))
        conn.commit()
        await update.message.reply_text(f"‚úÖ Posted draft #{did}")
    except Exception as e:
        await update.message.reply_text(f"‚ùå Channel post failed: {e}")
    finally:
        conn.close()


async def cmd_skip(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message:
        return
    if not context.args or not context.args[0].isdigit():
        await update.message.reply_text("Usage: /skip <draft_id>")
        return

    did = int(context.args[0])
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute("UPDATE drafts SET status='skipped' WHERE id=? AND status='pending'", (did,))
        conn.commit()
        await update.message.reply_text(f"üóë Skipped draft #{did}")
    finally:
        conn.close()


# ============================================================
# INIT + MAIN
# ============================================================

async def post_init(app: Application) -> None:
    if app.job_queue is None:
        raise RuntimeError('Install JobQueue: pip install -U "python-telegram-bot[job-queue]"')

    app.bot_data["openai_client"] = OpenAI(api_key=OPENAI_API_KEY)
    app.bot_data["db_conn"] = init_db()
    app.bot_data["next_ai_time"] = 0.0
    app.bot_data["last_run_time"] = 0.0

    await app.bot.send_message(
        chat_id=EDITOR_CHAT_ID,
        text="‚úÖ Bot started. Russian-only fallback enabled. Previews off. Links hidden. Hashtags enabled.",
        disable_web_page_preview=True
    )

    app.job_queue.run_repeating(
        rss_job_callback,
        interval=max(60, JOB_TICK_SECONDS),
        first=8,
        name="rss_tick"
    )


def main():
    acquire_lock_or_exit()
    print(f"[BOOT] env ok. job_tick={JOB_TICK_SECONDS}s db={DB_PATH}", flush=True)
    print("[BOOT] polling telegram‚Ä¶", flush=True)

    app = Application.builder().token(BOT_TOKEN).post_init(post_init).build()

    app.add_handler(CommandHandler("run", cmd_run))
    app.add_handler(CommandHandler("status", cmd_status))
    app.add_handler(CommandHandler("queue", cmd_queue))
    app.add_handler(CommandHandler("post", cmd_post))
    app.add_handler(CommandHandler("skip", cmd_skip))

    try:
        app.run_polling(drop_pending_updates=True)
    finally:
        try:
            conn = app.bot_data.get("db_conn")
            if conn:
                conn.close()
        except Exception:
            pass
        release_lock()


if __name__ == "__main__":
    main()
